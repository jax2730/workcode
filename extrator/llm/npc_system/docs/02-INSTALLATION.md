# 02 - å®‰è£…é…ç½®æŒ‡å—

> **é¢å‘å¯¹è±¡**: ç³»ç»Ÿå¼€å‘è€…ã€è¿ç»´äººå‘˜  
> **å‰ç½®çŸ¥è¯†**: LinuxåŸºç¡€ã€Pythonç¯å¢ƒç®¡ç†  
> **ç›¸å…³æ–‡æ¡£**: [01-å¿«é€Ÿå…¥é—¨](./01-QUICKSTART.md)

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

#### æœ€ä½é…ç½®
- **CPU**: 4æ ¸å¿ƒ
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 20GB å¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: ç¨³å®šçš„ç½‘ç»œè¿æ¥

#### æ¨èé…ç½®
- **CPU**: 8æ ¸å¿ƒæˆ–æ›´å¤š
- **å†…å­˜**: 16GB RAM æˆ–æ›´å¤š
- **å­˜å‚¨**: 50GB SSD
- **GPU**: NVIDIA GPU (å¯é€‰ï¼Œç”¨äºFAISSåŠ é€Ÿ)

### è½¯ä»¶è¦æ±‚

#### æ“ä½œç³»ç»Ÿ
- Ubuntu 20.04+ / Debian 11+
- CentOS 8+ / RHEL 8+
- macOS 11+
- Windows 10+ (WSL2æ¨è)

#### Pythonç¯å¢ƒ
- Python 3.8+
- pip 20.0+
- virtualenv æˆ– conda

#### æ•°æ®åº“
- SQLite 3.31+ (å†…ç½®)

#### å…¶ä»–ä¾èµ–
- Ollama (LLMæœåŠ¡)
- Git (ç‰ˆæœ¬æ§åˆ¶)

---

## ğŸš€ å®‰è£…æ­¥éª¤

### æ­¥éª¤1: å®‰è£…Pythonç¯å¢ƒ

#### Ubuntu/Debian
```bash
# æ›´æ–°åŒ…åˆ—è¡¨
sudo apt update

# å®‰è£…Python 3.10
sudo apt install python3.10 python3.10-venv python3.10-dev

# å®‰è£…pip
sudo apt install python3-pip

# éªŒè¯å®‰è£…
python3.10 --version
pip3 --version
```

#### CentOS/RHEL
```bash
# å®‰è£…EPELä»“åº“
sudo yum install epel-release

# å®‰è£…Python 3.10
sudo yum install python310 python310-devel

# å®‰è£…pip
sudo yum install python3-pip

# éªŒè¯å®‰è£…
python3.10 --version
pip3 --version
```

#### macOS
```bash
# ä½¿ç”¨Homebrewå®‰è£…
brew install python@3.10

# éªŒè¯å®‰è£…
python3.10 --version
pip3 --version
```

#### Windows (WSL2)
```bash
# åœ¨WSL2ä¸­æ‰§è¡ŒUbuntuçš„å®‰è£…æ­¥éª¤
# æˆ–è€…ç›´æ¥ä¸‹è½½Pythonå®‰è£…åŒ…
# https://www.python.org/downloads/
```

---

### æ­¥éª¤2: å®‰è£…Ollama

#### Linux
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# éªŒè¯å®‰è£…
ollama --version

# å¯åŠ¨OllamaæœåŠ¡
ollama serve &

# æ‹‰å–qwen2.5æ¨¡å‹
ollama pull qwen2.5:7b

# éªŒè¯æ¨¡å‹
ollama list
```

#### macOS
```bash
# ä¸‹è½½Ollamaå®‰è£…åŒ…
# https://ollama.com/download/mac

# æˆ–ä½¿ç”¨Homebrew
brew install ollama

# å¯åŠ¨æœåŠ¡
ollama serve &

# æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:7b
```

#### Windows
```bash
# ä¸‹è½½Ollamaå®‰è£…åŒ…
# https://ollama.com/download/windows

# å®‰è£…åï¼Œåœ¨PowerShellä¸­è¿è¡Œ
ollama pull qwen2.5:7b
```

---

### æ­¥éª¤3: å…‹éš†é¡¹ç›®

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url> LLM-Dialog
cd LLM-Dialog/OllamaSpace/SceneAgentServer

# æŸ¥çœ‹é¡¹ç›®ç»“æ„
ls -la
```

---

### æ­¥éª¤4: åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

#### ä½¿ç”¨venv
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.10 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Linux/macOS:
source venv/bin/activate

# Windows:
venv\Scripts\activate

# éªŒè¯
which python
python --version
```

#### ä½¿ç”¨conda
```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n npc-system python=3.10

# æ¿€æ´»ç¯å¢ƒ
conda activate npc-system

# éªŒè¯
which python
python --version
```

---

### æ­¥éª¤5: å®‰è£…Pythonä¾èµ–

#### æ ¸å¿ƒä¾èµ–
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install langchain-ollama langchain-core langchain-community

# å®‰è£…æ•°æ®å¤„ç†
pip install pandas openpyxl

# å®‰è£…å‘é‡æ£€ç´¢ (å¯é€‰)
pip install faiss-cpu  # CPUç‰ˆæœ¬
# æˆ–
pip install faiss-gpu  # GPUç‰ˆæœ¬ (éœ€è¦CUDA)

# å®‰è£…å…¶ä»–ä¾èµ–
pip install prompt_toolkit pyyaml python-dotenv

# éªŒè¯å®‰è£…
pip list | grep langchain
pip list | grep faiss
```

#### å®Œæ•´ä¾èµ–åˆ—è¡¨

åˆ›å»º `requirements.txt`:
```txt
# LLMå’ŒLangChain
langchain-ollama>=0.1.0
langchain-core>=0.1.0
langchain-community>=0.0.20

# æ•°æ®å¤„ç†
pandas>=2.0.0
openpyxl>=3.1.0
pyyaml>=6.0

# å‘é‡æ£€ç´¢ (å¯é€‰)
faiss-cpu>=1.7.4
# faiss-gpu>=1.7.4  # å¦‚æœæœ‰GPU

# å‘½ä»¤è¡Œå·¥å…·
prompt_toolkit>=3.0.0

# å·¥å…·åº“
python-dotenv>=1.0.0
requests>=2.31.0

# å¼€å‘å·¥å…· (å¯é€‰)
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0
```

å®‰è£…ï¼š
```bash
pip install -r requirements.txt
```

---

### æ­¥éª¤6: åˆå§‹åŒ–æ•°æ®ç›®å½•

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/OllamaSpace/SceneAgentServer

# åˆå§‹åŒ–æ•°æ®ç›®å½•
python -m extrator.llm.npc_system.init_data_dirs

# éªŒè¯ç›®å½•ç»“æ„
ls -la npc_data/
```

**é¢„æœŸè¾“å‡º**:
```
npc_data/
â”œâ”€â”€ memories/
â”‚   â”œâ”€â”€ working/
â”‚   â”œâ”€â”€ episodic/
â”‚   â”œâ”€â”€ semantic/
â”‚   â””â”€â”€ dialogues/
â”œâ”€â”€ databases/
â”‚   â”œâ”€â”€ dialogue_history.db
â”‚   â”œâ”€â”€ npc_memory.db
â”‚   â””â”€â”€ npc_relationship.db
â”œâ”€â”€ exports/
â”‚   â”œâ”€â”€ excel/
â”‚   â””â”€â”€ json/
â”œâ”€â”€ knowledge_base/
â”œâ”€â”€ rag_index/
â”œâ”€â”€ notes/
â”œâ”€â”€ configs/
â””â”€â”€ logs/
```

---

### æ­¥éª¤7: é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º.env
cat > .env << 'EOF'
# Ollamaé…ç½®
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b

# æ•°æ®ç›®å½•
NPC_DATA_DIR=./npc_data
NPC_CONFIG_DIR=./npc_configs

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FILE=./npc_data/logs/npc_system.log

# æ€§èƒ½é…ç½®
ENABLE_CACHE=true
CACHE_TTL=300
MAX_WORKERS=4

# RAGé…ç½®
ENABLE_FAISS=true
EMBEDDING_MODEL=nomic-embed-text

# æ•°æ®åº“é…ç½®
DB_PATH=./npc_data/databases/
EOF
```

---

### æ­¥éª¤8: éªŒè¯å®‰è£…

#### æµ‹è¯•Ollamaè¿æ¥
```bash
# æµ‹è¯•Ollama API
curl http://localhost:11434/api/tags

# æµ‹è¯•æ¨¡å‹
ollama run qwen2.5:7b "ä½ å¥½"
```

#### æµ‹è¯•Pythonç¯å¢ƒ
```bash
# æµ‹è¯•å¯¼å…¥
python -c "from langchain_ollama import ChatOllama; print('âœ… LangChain OK')"
python -c "import faiss; print('âœ… FAISS OK')"
python -c "import pandas; print('âœ… Pandas OK')"
```

#### è¿è¡Œç¤ºä¾‹
```bash
# è¿è¡ŒNPCå¯¹è¯æµ‹è¯•
python -m extrator.llm.npc_system.npc_chat

# æˆ–è¿è¡Œç¤ºä¾‹è„šæœ¬
python extrator/llm/npc_system/example.py
```

---

## âš™ï¸ é«˜çº§é…ç½®

### 1. GPUåŠ é€Ÿ (å¯é€‰)

#### å®‰è£…CUDA (NVIDIA GPU)
```bash
# Ubuntu
sudo apt install nvidia-cuda-toolkit

# éªŒè¯CUDA
nvcc --version
nvidia-smi

# å®‰è£…FAISS GPUç‰ˆæœ¬
pip uninstall faiss-cpu
pip install faiss-gpu
```

#### é…ç½®GPUä½¿ç”¨
```python
# åœ¨ä»£ç ä¸­å¯ç”¨GPU
import faiss

# æ£€æŸ¥GPUå¯ç”¨æ€§
if faiss.get_num_gpus() > 0:
    print(f"âœ… æ£€æµ‹åˆ° {faiss.get_num_gpus()} ä¸ªGPU")
    # ä½¿ç”¨GPUç´¢å¼•
    res = faiss.StandardGpuResources()
    index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
```

---

### 2. å¤šæ¨¡å‹é…ç½®

```bash
# æ‹‰å–å¤šä¸ªæ¨¡å‹
ollama pull qwen2.5:7b
ollama pull qwen2.5:14b
ollama pull llama2:13b

# åœ¨é…ç½®ä¸­æŒ‡å®šæ¨¡å‹
export OLLAMA_MODEL=qwen2.5:14b
```

---

### 3. åˆ†å¸ƒå¼éƒ¨ç½²

#### ä¸»èŠ‚ç‚¹é…ç½®
```bash
# ä¸»èŠ‚ç‚¹è¿è¡ŒOllama
ollama serve --host 0.0.0.0:11434

# é…ç½®é˜²ç«å¢™
sudo ufw allow 11434/tcp
```

#### å·¥ä½œèŠ‚ç‚¹é…ç½®
```bash
# å·¥ä½œèŠ‚ç‚¹è¿æ¥åˆ°ä¸»èŠ‚ç‚¹
export OLLAMA_HOST=http://master-node:11434

# æµ‹è¯•è¿æ¥
curl $OLLAMA_HOST/api/tags
```

---

### 4. æ€§èƒ½ä¼˜åŒ–

#### è°ƒæ•´Ollamaå‚æ•°
```bash
# è®¾ç½®å¹¶å‘æ•°
export OLLAMA_NUM_PARALLEL=4

# è®¾ç½®ä¸Šä¸‹æ–‡é•¿åº¦
export OLLAMA_CONTEXT_LENGTH=4096

# è®¾ç½®GPUå±‚æ•°
export OLLAMA_GPU_LAYERS=35
```

#### è°ƒæ•´Pythonå‚æ•°
```python
# åœ¨ä»£ç ä¸­é…ç½®
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="qwen2.5:7b",
    temperature=0.7,
    num_ctx=4096,        # ä¸Šä¸‹æ–‡é•¿åº¦
    num_gpu=35,          # GPUå±‚æ•°
    num_thread=8,        # çº¿ç¨‹æ•°
    repeat_penalty=1.1   # é‡å¤æƒ©ç½š
)
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: Ollamaè¿æ¥å¤±è´¥

**ç—‡çŠ¶**:
```
ConnectionError: Failed to connect to Ollama
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
ps aux | grep ollama

# å¦‚æœæ²¡æœ‰è¿è¡Œï¼Œå¯åŠ¨å®ƒ
ollama serve &

# æ£€æŸ¥ç«¯å£
netstat -tlnp | grep 11434

# æµ‹è¯•è¿æ¥
curl http://localhost:11434/api/tags
```

---

### é—®é¢˜2: æ¨¡å‹æœªæ‰¾åˆ°

**ç—‡çŠ¶**:
```
Error: model 'qwen2.5' not found
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list

# æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:7b

# éªŒè¯
ollama run qwen2.5:7b "æµ‹è¯•"
```

---

### é—®é¢˜3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull qwen2.5:3b

# æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
export OLLAMA_CONTEXT_LENGTH=2048

# æˆ–å¢åŠ swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

---

### é—®é¢˜4: FAISSå®‰è£…å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: Could not build wheels for faiss-cpu
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…ç¼–è¯‘ä¾èµ–
sudo apt install build-essential cmake

# æˆ–ä½¿ç”¨condaå®‰è£…
conda install -c conda-forge faiss-cpu

# æˆ–ä½¿ç”¨é¢„ç¼–è¯‘åŒ…
pip install faiss-cpu --no-cache-dir
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### è¿è¡ŒåŸºå‡†æµ‹è¯•
```bash
# æµ‹è¯•LLMå“åº”æ—¶é—´
python -c "
from langchain_ollama import ChatOllama
import time

llm = ChatOllama(model='qwen2.5:7b')
start = time.time()
response = llm.invoke('ä½ å¥½')
elapsed = time.time() - start
print(f'å“åº”æ—¶é—´: {elapsed:.2f}ç§’')
"

# æµ‹è¯•NPCç³»ç»Ÿå“åº”æ—¶é—´
python extrator/llm/npc_system/npc_chat.py
# è¾“å…¥å‡ æ¬¡å¯¹è¯åï¼Œä½¿ç”¨ 'stats' å‘½ä»¤æŸ¥çœ‹ç»Ÿè®¡
```

### é¢„æœŸæ€§èƒ½

| é…ç½® | LLMå“åº” | NPCç³»ç»Ÿå“åº” |
|------|---------|-------------|
| æœ€ä½é…ç½® (4æ ¸/8GB) | 3-5ç§’ | 5-8ç§’ |
| æ¨èé…ç½® (8æ ¸/16GB) | 1-2ç§’ | 2-4ç§’ |
| é«˜æ€§èƒ½ (16æ ¸/32GB+GPU) | 0.5-1ç§’ | 1-2ç§’ |

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [01-å¿«é€Ÿå…¥é—¨](./01-QUICKSTART.md) - å¿«é€Ÿä¸Šæ‰‹æŒ‡å—
- [03-æ¶æ„æ€»è§ˆ](./03-ARCHITECTURE_OVERVIEW.md) - ç³»ç»Ÿæ¶æ„
- [17-æ€§èƒ½ä¼˜åŒ–](../PERFORMANCE_ANALYSIS.md) - æ€§èƒ½ä¼˜åŒ–æŒ‡å—
- [19-FAQ](./19-FAQ.md) - å¸¸è§é—®é¢˜

---

## ğŸ“ è·å–å¸®åŠ©

### å®˜æ–¹èµ„æº
- Ollamaæ–‡æ¡£: https://ollama.com/docs
- LangChainæ–‡æ¡£: https://python.langchain.com/docs
- FAISSæ–‡æ¡£: https://github.com/facebookresearch/faiss

### ç¤¾åŒºæ”¯æŒ
- æäº¤Issue
- æŸ¥çœ‹Wiki
- åŠ å…¥è®¨è®ºç»„

---

æ­å–œï¼ä½ å·²ç»å®Œæˆäº†NPCæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰è£…é…ç½®ï¼ğŸ‰

ä¸‹ä¸€æ­¥ï¼šé˜…è¯» [01-å¿«é€Ÿå…¥é—¨](./01-QUICKSTART.md) å¼€å§‹ä½¿ç”¨ç³»ç»Ÿã€‚
