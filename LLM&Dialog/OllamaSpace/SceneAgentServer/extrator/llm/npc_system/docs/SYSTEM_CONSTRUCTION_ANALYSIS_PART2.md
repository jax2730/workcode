# å¯¹è¯ç³»ç»Ÿæ„å»ºé€»è¾‘è¯¦è§£ (ç¬¬2éƒ¨åˆ†)

> **æ¥ç»­**: SYSTEM_CONSTRUCTION_ANALYSIS.md  
> **æœ¬éƒ¨åˆ†å†…å®¹**: ä»é›¶æ„å»ºæŒ‡å—ã€å…±åŒç‚¹åˆ†æã€æœ€ä½³å®è·µ

---

## ğŸ—ï¸ ä»é›¶æ„å»ºæŒ‡å—

### æ„å»ºé¡ºåºå»ºè®®

```
é˜¶æ®µ1: åŸºç¡€å¯¹è¯ (1-2å¤©)
    â””â”€ å®ç° general_chat.py çº§åˆ«çš„åŠŸèƒ½

é˜¶æ®µ2: å·¥å…·è°ƒç”¨ (3-5å¤©)
    â””â”€ å®ç° llm.py çº§åˆ«çš„åŠŸèƒ½

é˜¶æ®µ3: å®Œæ•´NPCç³»ç»Ÿ (1-2å‘¨)
    â””â”€ å®ç° npc_chat.py çº§åˆ«çš„åŠŸèƒ½
```

---

## ğŸ“ é˜¶æ®µ1: æ„å»ºåŸºç¡€å¯¹è¯ç³»ç»Ÿ (general_chat.py)

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
pip install langchain langchain-ollama langchain-community
pip install sqlalchemy
```

### æ­¥éª¤2: åˆ›å»ºæœ€å°å¯¹è¯ç³»ç»Ÿ

```python
# minimal_chat.py
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

# 1. åˆå§‹åŒ–LLM
llm = ChatOllama(model="qwen2.5", temperature=0.7)

# 2. ç®€å•å¯¹è¯
def chat(message: str) -> str:
    response = llm.invoke([HumanMessage(content=message)])
    return response.content

# 3. æµ‹è¯•
if __name__ == "__main__":
    reply = chat("ä½ å¥½")
    print(reply)
```

**è¿™æ˜¯æœ€ç®€å•çš„å¯¹è¯ç³»ç»Ÿï¼Œåªéœ€3æ­¥ï¼**

### æ­¥éª¤3: æ·»åŠ å†å²è®°å½•

```python
# chat_with_history.py
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory

# 1. å†å²è®°å½•ç®¡ç†
history_cache = {}

def get_session_history(session_id: str):
    if session_id not in history_cache:
        history_cache[session_id] = SQLChatMessageHistory(
            session_id, 
            connection="sqlite:///chat.db"
        )
    return history_cache[session_id]

# 2. åˆ›å»ºå¸¦å†å²çš„å¯¹è¯é“¾
llm = ChatOllama(model="qwen2.5", temperature=0.7)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),
    MessagesPlaceholder(variable_name="input"),
])

chain = prompt | llm

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# 3. å¯¹è¯å‡½æ•°
def chat(message: str, session_id: str) -> str:
    response = chain_with_history.invoke(
        {"input": [HumanMessage(content=message)]},
        config={"configurable": {"session_id": session_id}}
    )
    return response.content
```

**ç°åœ¨æœ‰äº†å†å²è®°å½•åŠŸèƒ½ï¼**

### æ­¥éª¤4: æ·»åŠ æ¶ˆæ¯ä¿®å‰ª

```python
from langchain_core.messages import trim_messages

# é˜²æ­¢å†å²æ¶ˆæ¯è¿‡é•¿
trimmer = trim_messages(
    max_tokens=2000,
    strategy="last",
    token_counter=len,
    include_system=True,
)

# åœ¨é“¾ä¸­æ·»åŠ trimmer
chain = prompt | trimmer | llm
```

**å®Œæˆï¼è¿™å°±æ˜¯ general_chat.py çš„æ ¸å¿ƒé€»è¾‘ã€‚**

---

## ğŸ”§ é˜¶æ®µ2: æ„å»ºå·¥å…·è°ƒç”¨ç³»ç»Ÿ (llm.py)

### æ­¥éª¤1: å®šä¹‰å·¥å…·

```python
from pydantic import BaseModel, Field

# 1. å®šä¹‰å·¥å…·çš„æ•°æ®ç»“æ„
class WeatherQuery(BaseModel):
    """å¤©æ°”æŸ¥è¯¢å·¥å…·"""
    city: str = Field(..., description="åŸå¸‚åç§°")
    date: str = Field(default="ä»Šå¤©", description="æ—¥æœŸ")

# 2. ç»‘å®šå·¥å…·åˆ°LLM
llm = ChatOllama(model="qwen2.5")
llm_with_tools = llm.bind_tools([WeatherQuery])
```

### æ­¥éª¤2: åˆ›å»ºLangGraphå·¥ä½œæµ

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from typing import Annotated
from langgraph.graph.message import add_messages

# 1. å®šä¹‰çŠ¶æ€
class State(TypedDict):
    messages: Annotated[list, add_messages]

# 2. åˆ›å»ºå›¾
workflow = StateGraph(State)

# 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def call_model(state: State):
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

def call_tool(state: State):
    last_message = state["messages"][-1]
    tool_call = last_message.tool_calls[0]
    
    # æ‰§è¡Œå·¥å…·
    result = execute_weather_query(tool_call["args"])
    
    return {
        "messages": [ToolMessage(
            content=result,
            tool_call_id=tool_call["id"]
        )]
    }

# 4. æ·»åŠ èŠ‚ç‚¹
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tool)

# 5. æ·»åŠ è¾¹
def should_continue(state: State):
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "tools"
    return END

workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_edge("tools", "agent")
workflow.add_edge(START, "agent")

# 6. ç¼–è¯‘
graph = workflow.compile()
```

### æ­¥éª¤3: ä½¿ç”¨å·¥ä½œæµ

```python
def chat(message: str, session_id: str):
    result = graph.invoke(
        {"messages": [HumanMessage(content=message)]},
        config={"configurable": {"session_id": session_id}}
    )
    return result["messages"][-1].content
```

**å®Œæˆï¼è¿™å°±æ˜¯ llm.py çš„æ ¸å¿ƒé€»è¾‘ã€‚**

---

## ğŸ® é˜¶æ®µ3: æ„å»ºNPCå¯¹è¯ç³»ç»Ÿ (npc_chat.py)

### æ­¥éª¤1: è®¾è®¡NPCæ•°æ®ç»“æ„

```python
from dataclasses import dataclass
from typing import List

@dataclass
class NPCPersonality:
    """NPCäººè®¾"""
    name: str
    role: str
    age: int = 30
    traits: List[str] = None
    background: str = ""
    speech_style: str = ""
    knowledge: List[str] = None
    secrets: List[str] = None
    greeting: str = ""
```

### æ­¥éª¤2: åˆ›å»ºè®°å¿†ç³»ç»Ÿ

```python
class MemorySystem:
    """ç®€åŒ–çš„è®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.memories = []
    
    def add_memory(self, content: str, memory_type: str, importance: float):
        """æ·»åŠ è®°å¿†"""
        memory = {
            "content": content,
            "type": memory_type,
            "importance": importance,
            "timestamp": datetime.now()
        }
        self.memories.append(memory)
    
    def search(self, query: str, limit: int = 5):
        """æ£€ç´¢è®°å¿† (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨å‘é‡æ£€ç´¢)"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        results = []
        for memory in self.memories:
            if query.lower() in memory["content"].lower():
                results.append(memory)
        return results[:limit]
```

### æ­¥éª¤3: åˆ›å»ºå¥½æ„Ÿåº¦ç³»ç»Ÿ

```python
class RelationshipManager:
    """å¥½æ„Ÿåº¦ç®¡ç†"""
    
    def __init__(self):
        self.relationships = {}  # {(npc_id, player_id): score}
    
    def get_affinity(self, npc_id: str, player_id: str) -> int:
        """è·å–å¥½æ„Ÿåº¦"""
        key = (npc_id, player_id)
        return self.relationships.get(key, 0)
    
    def update_affinity(self, npc_id: str, player_id: str, delta: int):
        """æ›´æ–°å¥½æ„Ÿåº¦"""
        key = (npc_id, player_id)
        current = self.relationships.get(key, 0)
        self.relationships[key] = max(0, min(100, current + delta))
    
    def get_level(self, score: int) -> str:
        """è·å–å¥½æ„Ÿåº¦ç­‰çº§"""
        if score < 20: return "é™Œç”Ÿ"
        elif score < 40: return "è®¤è¯†"
        elif score < 60: return "å‹å¥½"
        elif score < 80: return "ä¿¡ä»»"
        else: return "æŒšå‹"
```

### æ­¥éª¤4: åˆ›å»ºNPCAgent

```python
class NPCAgent:
    """NPCæ™ºèƒ½ä½“"""
    
    def __init__(self, npc_id: str, personality: NPCPersonality, llm):
        self.npc_id = npc_id
        self.personality = personality
        self.llm = llm
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.memory = MemorySystem()
        self.relationship = RelationshipManager()
        self.sessions = {}  # ä¼šè¯å†å²
    
    def chat(self, player_id: str, message: str) -> dict:
        """å¯¹è¯å¤„ç†"""
        # 1. æ£€ç´¢è®°å¿†
        memories = self.memory.search(message, limit=3)
        
        # 2. è·å–å¥½æ„Ÿåº¦
        affinity_score = self.relationship.get_affinity(self.npc_id, player_id)
        affinity_level = self.relationship.get_level(affinity_score)
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(memories, affinity_level)
        
        # 4. ç”Ÿæˆå›å¤
        reply = self._generate_reply(context, message)
        
        # 5. æ›´æ–°å¥½æ„Ÿåº¦
        delta = self._calculate_affinity_change(message)
        self.relationship.update_affinity(self.npc_id, player_id, delta)
        
        # 6. ä¿å­˜è®°å¿†
        self.memory.add_memory(f"ç©å®¶è¯´: {message}", "episodic", 0.6)
        self.memory.add_memory(f"æˆ‘å›å¤: {reply}", "episodic", 0.5)
        
        return {
            "reply": reply,
            "affinity": {
                "score": affinity_score + delta,
                "level": self.relationship.get_level(affinity_score + delta)
            }
        }
    
    def _build_context(self, memories, affinity_level):
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context = f"ä½ æ˜¯{self.personality.name}ï¼Œ{self.personality.role}ã€‚\n"
        context += f"èƒŒæ™¯: {self.personality.background}\n"
        context += f"è¯´è¯é£æ ¼: {self.personality.speech_style}\n"
        context += f"å½“å‰å¥½æ„Ÿåº¦: {affinity_level}\n"
        
        if memories:
            context += "\nç›¸å…³è®°å¿†:\n"
            for mem in memories:
                context += f"- {mem['content']}\n"
        
        return context
    
    def _generate_reply(self, context, message):
        """ç”Ÿæˆå›å¤"""
        prompt = f"{context}\n\nç”¨æˆ·: {message}\n{self.personality.name}:"
        response = self.llm.invoke([HumanMessage(content=prompt)])
        return response.content
    
    def _calculate_affinity_change(self, message):
        """è®¡ç®—å¥½æ„Ÿåº¦å˜åŒ–"""
        # ç®€å•çš„æƒ…æ„Ÿåˆ†æ
        positive_words = ["è°¢è°¢", "æ„Ÿè°¢", "å¸®åŠ©", "å¥½"]
        negative_words = ["è®¨åŒ", "çƒ¦", "æ»š"]
        
        delta = 0
        for word in positive_words:
            if word in message:
                delta += 2
        for word in negative_words:
            if word in message:
                delta -= 3
        
        return delta
```

### æ­¥éª¤5: åˆ›å»ºNPCç®¡ç†å™¨

```python
class NPCManager:
    """NPCç®¡ç†å™¨"""
    
    def __init__(self, llm):
        self.llm = llm
        self.npcs = {}  # {npc_id: NPCAgent}
    
    def register_npc(self, npc_id: str, personality: NPCPersonality):
        """æ³¨å†ŒNPC"""
        npc = NPCAgent(npc_id, personality, self.llm)
        self.npcs[npc_id] = npc
    
    def get_npc(self, npc_id: str) -> NPCAgent:
        """è·å–NPC"""
        return self.npcs.get(npc_id)
    
    def chat(self, npc_id: str, player_id: str, message: str) -> dict:
        """ä¸NPCå¯¹è¯"""
        npc = self.get_npc(npc_id)
        if not npc:
            return {"success": False, "error": f"NPCä¸å­˜åœ¨: {npc_id}"}
        
        result = npc.chat(player_id, message)
        result["success"] = True
        result["npc_name"] = npc.personality.name
        return result
```

**å®Œæˆï¼è¿™å°±æ˜¯ npc_chat.py çš„æ ¸å¿ƒé€»è¾‘ã€‚**

---

## ğŸ” å…±åŒç‚¹åˆ†æ

### 1. æŠ€æœ¯æ ˆå…±åŒç‚¹

æ‰€æœ‰ä¸‰ä¸ªç³»ç»Ÿéƒ½ä½¿ç”¨ï¼š

```python
# 1. LangChainæ¡†æ¶
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 2. Ollamaä½œä¸ºLLMåç«¯
llm = ChatOllama(model="qwen2.5")

# 3. SQLiteå­˜å‚¨å†å²
from langchain_community.chat_message_histories import SQLChatMessageHistory

# 4. ä¼šè¯ç®¡ç†
history_cache = {}
def get_session_history(session_id):
    if session_id not in history_cache:
        history_cache[session_id] = SQLChatMessageHistory(...)
    return history_cache[session_id]
```

### 2. æ¶æ„æ¨¡å¼å…±åŒç‚¹

#### æ¨¡å¼1: åˆå§‹åŒ– â†’ å¯¹è¯ â†’ è¿”å›

```python
# æ‰€æœ‰ç³»ç»Ÿéƒ½éµå¾ªè¿™ä¸ªåŸºæœ¬æ¨¡å¼

# 1. åˆå§‹åŒ–
llm = ChatOllama(...)
chain = create_chain(llm)

# 2. å¯¹è¯å‡½æ•°
def chat(message: str, session_id: str) -> dict:
    result = chain.invoke(...)
    return format_result(result)

# 3. è¿”å›æ ‡å‡†æ ¼å¼
{
    "success": 1,
    "msg": "å›å¤å†…å®¹",
    "data": {...}  # å¯é€‰
}
```

#### æ¨¡å¼2: ä¼šè¯ç®¡ç†

```python
# æ‰€æœ‰ç³»ç»Ÿéƒ½ä½¿ç”¨session_idç®¡ç†ä¼šè¯

# llm.py
session_id = "#".join([ipAddr, str(uuid.uuid4())])

# general_chat.py
session_id = "general#" + "#".join([ipAddr, str(uuid.uuid4())])

# npc_chat.py
session_id = f"session_{player_id}_{datetime.now()}"
```

#### æ¨¡å¼3: Djangoé›†æˆ

```python
# æ‰€æœ‰ç³»ç»Ÿéƒ½æä¾›Djangoè§†å›¾

@csrf_exempt
def chat_view(request):
    # 1. è·å–session_id
    session_id = request.COOKIES.get("session_id")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥
    message = request.body.decode('utf-8')
    
    # 3. è°ƒç”¨å¯¹è¯ç³»ç»Ÿ
    result = chat_system.chat(message, session_id)
    
    # 4. è¿”å›JSON
    return JsonResponse(result)
```

### 3. æ•°æ®æµå…±åŒç‚¹

```
ç”¨æˆ·è¾“å…¥ â†’ ä¼šè¯ç®¡ç† â†’ å†å²åŠ è½½ â†’ ä¸Šä¸‹æ–‡æ„å»º â†’ LLMæ¨ç† â†’ å†å²ä¿å­˜ â†’ è¿”å›ç»“æœ
```

æ‰€æœ‰ç³»ç»Ÿéƒ½éµå¾ªè¿™ä¸ªåŸºæœ¬æ•°æ®æµï¼Œåªæ˜¯åœ¨"ä¸Šä¸‹æ–‡æ„å»º"é˜¶æ®µçš„å¤æ‚åº¦ä¸åŒï¼š

- **general_chat.py**: ç³»ç»Ÿæç¤ºè¯ + å†å²æ¶ˆæ¯
- **llm.py**: ç³»ç»Ÿæç¤ºè¯ + å†å²æ¶ˆæ¯ + å·¥å…·å®šä¹‰
- **npc_chat.py**: äººè®¾ + è®°å¿† + çŸ¥è¯† + å¥½æ„Ÿåº¦ + å†å²æ¶ˆæ¯

---

## ğŸ¯ å·®å¼‚ç‚¹åˆ†æ

### 1. æ ¸å¿ƒç›®æ ‡å·®å¼‚

```
llm.py
â”œâ”€ ç›®æ ‡: å®Œæˆç‰¹å®šä»»åŠ¡ (æ˜Ÿçƒç¯å¢ƒè®¾ç½®)
â”œâ”€ è¾“å‡º: ç»“æ„åŒ–æ•°æ®
â””â”€ è¯„ä»·: ä»»åŠ¡å®Œæˆåº¦

general_chat.py
â”œâ”€ ç›®æ ‡: è‡ªç”±å¯¹è¯
â”œâ”€ è¾“å‡º: è‡ªç„¶è¯­è¨€
â””â”€ è¯„ä»·: å¯¹è¯è´¨é‡

npc_chat.py
â”œâ”€ ç›®æ ‡: è§’è‰²æ‰®æ¼”
â”œâ”€ è¾“å‡º: ç¬¦åˆäººè®¾çš„å›å¤
â””â”€ è¯„ä»·: äººè®¾ä¸€è‡´æ€§ + è®°å¿†è¿è´¯æ€§
```

### 2. çŠ¶æ€ç®¡ç†å·®å¼‚

```python
# llm.py - ä½¿ç”¨LangGraphç®¡ç†å¤æ‚çŠ¶æ€
workflow = StateGraph(State)
workflow.add_node("info", info_chain)
workflow.add_node("add_tool_message", add_tool_message)
workflow.add_conditional_edges(...)

# general_chat.py - æ— çŠ¶æ€ç®¡ç†ï¼Œåªæœ‰å†å²
# (å†å²ç”±LangChainè‡ªåŠ¨ç®¡ç†)

# npc_chat.py - ä½¿ç”¨NPCAgentç®¡ç†å¤šç§çŠ¶æ€
class NPCAgent:
    def __init__(self):
        self.memory = MemorySystem()
        self.relationship = RelationshipManager()
        self.sessions = {}
        self.dialogue_storage = DialogueStorage()
```

### 3. ä¸Šä¸‹æ–‡æ„å»ºå·®å¼‚

```python
# llm.py - ä»»åŠ¡å¯¼å‘çš„ä¸Šä¸‹æ–‡
context = f"""
**ä»£ç†ä»»åŠ¡:**
è¯·ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ¸©åº¦ã€æ¹¿åº¦ã€é¢œè‰²...

**æ­¥éª¤:**
1. æå–æ¸©åº¦
2. æå–æ¹¿åº¦
3. ç¡®è®¤ä¿¡æ¯
4. è°ƒç”¨å·¥å…·
"""

# general_chat.py - ç®€å•çš„ç³»ç»Ÿæç¤ºè¯
context = """
ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚
è¯·ç”¨ç®€æ´ã€å‡†ç¡®çš„è¯­è¨€å›å¤ç”¨æˆ·ã€‚
"""

# npc_chat.py - å¤šç»´åº¦çš„ä¸Šä¸‹æ–‡
context = f"""
ä½ æ˜¯{npc.name}ï¼Œ{npc.role}ã€‚
èƒŒæ™¯: {npc.background}
æ€§æ ¼: {npc.traits}
è¯´è¯é£æ ¼: {npc.speech_style}

ç›¸å…³è®°å¿†:
{memories}

ç›¸å…³çŸ¥è¯†:
{knowledge}

å½“å‰å¥½æ„Ÿåº¦: {affinity_level}
å¯åˆ†äº«çš„ç§˜å¯†: {secrets}
"""
```

### 4. è¾“å‡ºå¤„ç†å·®å¼‚

```python
# llm.py - è§£æç»“æ„åŒ–è¾“å‡º
if last_message.tool_calls:
    args = last_message.tool_calls[-1].get('args')
    result["data"] = process_planet_data(args)

# general_chat.py - ç›´æ¥è¿”å›æ–‡æœ¬
result["msg"] = response.content

# npc_chat.py - è¿”å›å¤šç»´åº¦ä¿¡æ¯
return {
    "reply": reply,
    "affinity": {
        "score": score,
        "level": level
    },
    "npc_name": npc.name,
    "session_id": session_id
}
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ç³»ç»Ÿ

**ä½¿ç”¨ general_chat.py å½“ä½ éœ€è¦**:
- âœ… å¿«é€ŸåŸå‹
- âœ… ç®€å•é—®ç­”
- âœ… æ€§èƒ½åŸºå‡†
- âŒ ä¸éœ€è¦å¤æ‚åŠŸèƒ½

**ä½¿ç”¨ llm.py å½“ä½ éœ€è¦**:
- âœ… ç»“æ„åŒ–ä¿¡æ¯æå–
- âœ… å·¥å…·/APIè°ƒç”¨
- âœ… å¤šæ­¥éª¤ä»»åŠ¡
- âŒ ä¸éœ€è¦è§’è‰²æ‰®æ¼”

**ä½¿ç”¨ npc_chat.py å½“ä½ éœ€è¦**:
- âœ… æ¸¸æˆNPC
- âœ… è§’è‰²æ‰®æ¼”
- âœ… é•¿æœŸè®°å¿†
- âœ… å…³ç³»ç®¡ç†
- âŒ å¯ä»¥æ¥å—è¾ƒæ…¢çš„å“åº”

### 2. æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# 1. ä½¿ç”¨ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
def get_npc_context(npc_id: str):
    # ç¼“å­˜NPCä¸Šä¸‹æ–‡
    pass

# 2. æ‰¹é‡å¤„ç†
def batch_chat(messages: List[str]):
    # æ‰¹é‡è°ƒç”¨LLM
    pass

# 3. å¼‚æ­¥å¤„ç†
async def chat_async(message: str):
    # å¼‚æ­¥è°ƒç”¨
    pass

# 4. å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
trimmer = trim_messages(max_tokens=2000)
```

### 3. é”™è¯¯å¤„ç†

```python
def chat(message: str, session_id: str) -> dict:
    result = {"success": 1, "msg": ""}
    
    try:
        # 1. éªŒè¯è¾“å…¥
        if not message or not session_id:
            raise ValueError("ç¼ºå°‘å¿…è¦å‚æ•°")
        
        # 2. è°ƒç”¨LLM
        response = llm.invoke(...)
        result["msg"] = response.content
        
    except ValueError as e:
        result["success"] = 0
        result["msg"] = f"å‚æ•°é”™è¯¯: {e}"
    except Exception as e:
        result["success"] = 0
        result["msg"] = f"ç³»ç»Ÿé”™è¯¯: {e}"
        # è®°å½•æ—¥å¿—
        logger.error(f"Chat error: {e}")
    
    return result
```

### 4. æµ‹è¯•å»ºè®®

```python
# 1. å•å…ƒæµ‹è¯•
def test_chat():
    result = chat("ä½ å¥½", "test_session")
    assert result["success"] == 1
    assert len(result["msg"]) > 0

# 2. æ€§èƒ½æµ‹è¯•
def test_performance():
    start = time.time()
    chat("ä½ å¥½", "test_session")
    duration = time.time() - start
    assert duration < 5.0  # 5ç§’å†…å“åº”

# 3. é›†æˆæµ‹è¯•
def test_django_integration():
    response = client.post('/chat/', data={"message": "ä½ å¥½"})
    assert response.status_code == 200
```

---

## ğŸ“š æ€»ç»“

### æ„å»ºå¯¹è¯ç³»ç»Ÿçš„å…³é”®æ­¥éª¤

1. **é€‰æ‹©åˆé€‚çš„å¤æ‚åº¦**: ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥å¢åŠ åŠŸèƒ½
2. **ä½¿ç”¨æ ‡å‡†æ¡†æ¶**: LangChainæä¾›äº†å®Œæ•´çš„å·¥å…·é“¾
3. **ç®¡ç†å¥½ä¼šè¯**: session_idæ˜¯æ ¸å¿ƒ
4. **ä¼˜åŒ–æ€§èƒ½**: ç¼“å­˜ã€æ‰¹é‡ã€å¼‚æ­¥
5. **å®Œå–„é”™è¯¯å¤„ç†**: ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
6. **é›†æˆDjango**: æä¾›Web APIæ¥å£

### ä¸‰ä¸ªç³»ç»Ÿçš„æ¼”è¿›è·¯å¾„

```
general_chat.py (åŸºç¡€)
    â†“ æ·»åŠ å·¥å…·è°ƒç”¨
llm.py (ä¸­çº§)
    â†“ æ·»åŠ è®°å¿†ã€çŸ¥è¯†ã€å¥½æ„Ÿåº¦
npc_chat.py (é«˜çº§)
```

### æ¨èå­¦ä¹ è·¯å¾„

1. **ç¬¬1å‘¨**: å®ç° general_chat.pyï¼Œç†è§£åŸºç¡€å¯¹è¯
2. **ç¬¬2å‘¨**: å®ç° llm.pyï¼Œç†è§£å·¥å…·è°ƒç”¨å’ŒçŠ¶æ€ç®¡ç†
3. **ç¬¬3-4å‘¨**: å®ç° npc_chat.pyï¼Œç†è§£å®Œæ•´çš„NPCç³»ç»Ÿ

---

## ğŸ“ å»¶ä¼¸é˜…è¯»

- LangChainå®˜æ–¹æ–‡æ¡£: https://python.langchain.com/
- LangGraphæ•™ç¨‹: https://langchain-ai.github.io/langgraph/
- Ollamaæ–‡æ¡£: https://ollama.ai/
- Django REST Framework: https://www.django-rest-framework.org/

---

**ç¥ä½ æ„å»ºæˆåŠŸï¼** ğŸš€
